apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  name: ccd-logstash
  annotations:
    hmcts.github.com/prod-automated: disabled
spec:
  releaseName: ccd-logstash
  rollback:
    enable: true
    retry: true
  chart:
    repository: https://helm.elastic.co
    name: logstash
    version: 6.8.13
  values:
    image: "hmctspublic.azurecr.io/imported/logstash"
    imageTag: "7.10.1"
    imagePullPolicy: "IfNotPresent"
    logstashJavaOpts: "-Xms4g -Xmx8g"
    resources:
      requests:
        cpu: "2000m"
        memory: "4Gi"
      limits:
        cpu: "4000m"
        memory: "8Gi"
    envFrom:
    - secretRef:
        name: logstash-secret
    extraInitContainers: |
      - name: download-postgres-jdbc
        image: hmctspublic.azurecr.io/curl:7.70.0
        command: ['curl', '-L', 'https://jdbc.postgresql.org/download/postgresql-42.2.18.jar', '-o', '/logstash-lib/postgresql.jar']
        volumeMounts:
        - name: logstash-lib
          mountPath: /logstash-lib
    extraVolumes: |
      - name: logstash-lib
        emptyDir: {}
    extraVolumeMounts: |
      - name: logstash-lib
        mountPath: /usr/share/logstash/ccd
    logstashConfig:
      logstash.yml: |
        http.host: 0.0.0.0
        xpack.monitoring.enabled: true
        xpack.monitoring.elasticsearch.hosts: ["${ES_HOSTS}"]
        queue.type: persisted
        dead_letter_queue.enable: true
      pipelines.yml: |
        - pipeline.id: main
          path.config: "/usr/share/logstash/pipeline/{01_input,02_filter,03_output}.conf"
          pipeline.workers: 4
          pipeline.batch.size: 500
          queue.type: persisted
        - pipeline.id: index-dead-letter-to-es
          path.config: "/usr/share/logstash/pipeline/dead_letter_indexing_pipeline.conf"
          pipeline.workers: 1
          dead_letter_queue.enable: false
    logstashPipeline:
      01_input.conf: |
        input  {
          jdbc {
            jdbc_connection_string => "${DATA_STORE_URL}"
            jdbc_user => "${DATA_STORE_USER}"
            jdbc_password => "${DATA_STORE_PASS}"
            jdbc_validate_connection => true
            jdbc_driver_library => "/usr/share/logstash/ccd/postgresql.jar"
            jdbc_driver_class => "org.postgresql.Driver"
            jdbc_default_timezone => "UTC"
            parameters => {
                        "divorcej" => "DIVORCE"
                        "cmcj" => "CMC"
                        "probatej" => "PROBATE"
                        "ethosj" => "EMPLOYMENT"
                        "sscsj" => "SSCS"
                      }
            statement => "UPDATE case_data SET marked_by_logstash = true WHERE marked_by_logstash = false AND jurisdiction != :divorcej AND jurisdiction != :cmcj AND jurisdiction != :probatej AND jurisdiction != :sscsj AND jurisdiction != :ethosj RETURNING id, created_date, last_modified, jurisdiction, case_type_id, state, last_state_modified_date, data::TEXT as json_data, data_classification::TEXT as json_data_classification, reference, security_classification, supplementary_data::TEXT as json_supplementary_data"
            clean_run => false
            schedule => "* * * * * *"
          }
        }
      02_filter.conf: |
        filter{
          json{
            source => "json_data"
            target => "data"
            remove_field => ["json_data"]
          }
          json{
            source => "json_data_classification"
            target => "data_classification"
            remove_field => ["json_data_classification"]
          }
          json {
              source => "json_supplementary_data"
              target => "supplementary_data"
              remove_field => ["json_supplementary_data"]
            }
          mutate {
              add_field => { "index_id" => "%{case_type_id}_cases" }
            }
          mutate {
            lowercase => [ "index_id" ]
          }
        }
      03_output.conf: |
        output {
            elasticsearch {
                hosts => ["${ES_HOSTS}"]
                sniffing => false
                index => "%{[index_id]}"
                document_type => "_doc"
                document_id => "%{id}"
                timeout => 60
            }
        }
      dead_letter_indexing_pipeline.conf: |
        input {
          dead_letter_queue {
            path => "${LOGSTASH_HOME}/data/dead_letter_queue"
            commit_offsets => true
            pipeline_id => "main"
          }
        }

        filter {
          # capture the entire event, and write it to a new field; we'll call that field `failed_case`
          ruby {
            code => "event.set('failed_case', event.to_json())"
          }

          # prune every field off the event except for the one we've just created. Note that this does not prune event metadata.
          prune {
            whitelist_names => [ "^failed_case$" ]
          }

          ruby {
            code => "event.set('timestamp', event.get('[@metadata][dead_letter_queue][entry_time]'))"
          }

          # pull useful information out of the event metadata provided by the dead letter queue, and add it to the new event.
          mutate {
            add_field => {
              "reason" => "%{[@metadata][dead_letter_queue][reason]}"
            }
          }
        }

        output {
          elasticsearch {
            hosts => ["${ES_HOSTS}"]
            sniffing => false
            index => ".logstash_dead_letter"
          }
        }