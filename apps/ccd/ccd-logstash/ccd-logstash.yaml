apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: ccd-logstash
spec:
  releaseName: ccd-logstash
  values:
    image: "hmctspublic.azurecr.io/imported/logstash/logstash"
    imageTag: "7.16.1"
    imagePullPolicy: "IfNotPresent"
    logstashJavaOpts: "-Xms4g -Xmx8g"
    resources:
      requests:
        cpu: "2000m"
        memory: "4Gi"
      limits:
        cpu: "4000m"
        memory: "8Gi"
    envFrom:
      - secretRef:
          name: logstash-secret
    extraInitContainers: |
      - name: download-postgres-jdbc
        image: hmctspublic.azurecr.io/curl:7.70.0
        command: ['curl', '-L', 'https://jdbc.postgresql.org/download/postgresql-42.3.3.jar', '-o', '/logstash-lib/postgresql.jar']
        volumeMounts:
        - name: logstash-lib
          mountPath: /logstash-lib
    extraVolumes: |
      - name: logstash-lib
        emptyDir: {}
    extraVolumeMounts: |
      - name: logstash-lib
        mountPath: /usr/share/logstash/ccd
    logstashConfig:
      logstash.yml: |
        http.host: 0.0.0.0
        xpack.monitoring.enabled: true
        xpack.monitoring.elasticsearch.hosts: ["${ES_HOSTS}"]
        queue.type: persisted
        dead_letter_queue.enable: true
        log.level: trace
      pipelines.yml: |
        - pipeline.id: main
          path.config: "/usr/share/logstash/pipeline/{01_input,02_filter,03_output}.conf"
          pipeline.workers: 4
          pipeline.batch.size: 500
          queue.type: persisted
        - pipeline.id: index-dead-letter-to-es
          path.config: "/usr/share/logstash/pipeline/dead_letter_indexing_pipeline.conf"
          pipeline.workers: 1
          dead_letter_queue.enable: false
    logstashPipeline:
      01_input.conf: |
        input  {
          jdbc {
            jdbc_connection_string => "${DATA_STORE_URL}"
            jdbc_user => "${DATA_STORE_USER}"
            jdbc_password => "${DATA_STORE_PASS}"
            jdbc_validate_connection => true
            jdbc_validation_timeout => "1"
            jdbc_driver_library => "/usr/share/logstash/ccd/postgresql.jar"
            jdbc_driver_class => "org.postgresql.Driver"
            jdbc_default_timezone => "UTC"
            parameters => {
                        "divorcej" => "DIVORCE"
                        "cmcj" => "CMC"
                        "probatej" => "PROBATE"
                        "ethosj" => "EMPLOYMENT"
                        "sscsj" => "SSCS"
                      }
            statement => "UPDATE case_data SET marked_by_logstash = true WHERE marked_by_logstash = false AND jurisdiction != :divorcej AND jurisdiction != :cmcj AND jurisdiction != :probatej AND jurisdiction != :sscsj AND jurisdiction != :ethosj RETURNING id, created_date, last_modified, jurisdiction, case_type_id, state, last_state_modified_date, data::TEXT as json_data, data_classification::TEXT as json_data_classification, reference, security_classification, supplementary_data::TEXT as json_supplementary_data"
            clean_run => false
            schedule => "* * * * * *"
          }
        }
      02_filter.conf: |
        filter{
          json{
             	source => "json_data"
             	target => "data"
             	remove_field => ["json_data"]
             }

             json{
                  source => "json_supplementary_data"
                  target => "supplementary_data"
                  remove_field => ["json_supplementary_data"]
             }

             json{
                  source => "json_data_classification"
                  target => "data_classification"
                  remove_field => ["json_data_classification"]
             }

             if [data][SearchCriteria] {
                 clone {
                     clones => ["SearchCriteria"]
                 }
             }

             if [type] == "SearchCriteria" {
                 if [data][SearchCriteria] {
                     mutate {
                        rename => {"[data][SearchCriteria]" => "[data_new][SearchCriteria]" }
                     }
                 }
                 if [data][caseManagementLocation] {
                     mutate {
                        rename => {"[data][caseManagementLocation]" => "[data_new][caseManagementLocation]" }
                     }
                 }
                 if [data][CaseAccessCategory] {
                    mutate {
                       rename => {"[data][CaseAccessCategory]" => "[data_new][CaseAccessCategory]" }
                    }
                 }
                 if [data][caseNameHmctsInternal] {
                     mutate {
                        rename => {"[data][caseNameHmctsInternal]" => "[data_new][caseNameHmctsInternal]" }
                     }
                 }
                 if [data][caseManagementCategory] {
                     mutate {
                        rename => {"[data][caseManagementCategory]" => "[data_new][caseManagementCategory]" }
                     }
                 }
                 if [supplementary_data][HMCTSServiceId] {
                     mutate {
                        rename => {"[supplementary_data][HMCTSServiceId]" => "[supplementary_data_new][HMCTSServiceId]" }
                     }
                 }
                 if [data_classification][SearchCriteria] {
                     mutate {
                        rename => {"[data_classification][SearchCriteria]" => "[data_classification_new][SearchCriteria]" }
                     }
                 }
                 if [data_classification][CaseAccessCategory] {
                    mutate {
                         rename => {"[data_classification][CaseAccessCategory]" => "[data_classification_new][CaseAccessCategory]" }
                    }
                 }
                 if [data_classification][caseManagementLocation] {
                    mutate {
                       rename => {"[data_classification][caseManagementLocation]" => "[data_classification_new][caseManagementLocation]" }
                    }
                 }
                 if [data_classification][caseNameHmctsInternal] {
                     mutate {
                        rename => {"[data_classification][caseNameHmctsInternal]" => "[data_classification_new][caseNameHmctsInternal]" }
                     }
                 }

                 if [data_classification][caseManagementCategory] {
                     mutate {
                        rename => {"[data_classification][caseManagementCategory]" => "[data_classification_new][caseManagementCategory]" }
                     }
                 }
                 mutate { remove_field =>[ "data" ,"supplementary_data", "data_classification", "last_state_modified_date", "type","last_modified", "created_date" ] }

                 mutate {
                         rename => { "[data_new]" => "data" }
                         rename => { "[supplementary_data_new]"  => "supplementary_data" }
                         rename => { "[data_classification_new]"  => "data_classification" }
                 }

                 mutate {
                    add_field => { "index_id" => "global_search" }
                 }
                 mutate {
                    lowercase => [ "index_id" ]
                 }
             } else {
                 mutate {
                     add_field => { "index_id" => "%{case_type_id}_cases" }
               	}
              mutate {
          	    lowercase => [ "index_id" ]
              }
             }
        }
      03_output.conf: |
        output {
            elasticsearch {
                hosts => ["${ES_HOSTS}"]
                sniffing => false
                index => "%{[index_id]}"
                document_type => "_doc"
                document_id => "%{id}"
                timeout => 60
            }
        }
      dead_letter_indexing_pipeline.conf: |
        input {
          dead_letter_queue {
            path => "${LOGSTASH_HOME}/data/dead_letter_queue"
            commit_offsets => true
            pipeline_id => "main"
          }
        }

        filter {
          # capture the entire event, and write it to a new field; we'll call that field `failed_case`
          ruby {
            code => "event.set('failed_case', event.to_json())"
          }

          # prune every field off the event except for the one we've just created. Note that this does not prune event metadata.
          prune {
            whitelist_names => [ "^failed_case$" ]
          }

          ruby {
            code => "event.set('timestamp', event.get('[@metadata][dead_letter_queue][entry_time]'))"
          }

          # pull useful information out of the event metadata provided by the dead letter queue, and add it to the new event.
          mutate {
            add_field => {
              "reason" => "%{[@metadata][dead_letter_queue][reason]}"
            }
          }
        }

        output {
          elasticsearch {
            hosts => ["${ES_HOSTS}"]
            sniffing => false
            index => ".logstash_dead_letter"
          }
        }
  chart:
    spec:
      chart: logstash
      version: 6.8.22
      sourceRef:
        kind: HelmRepository
        name: elastic
        namespace: ccd
